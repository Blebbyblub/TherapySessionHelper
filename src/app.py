# app.py (COMPLETELY FIXED VERSION WITH COMPLETE SESSION AND UPLOAD)
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import tempfile
import os
import time
from datetime import datetime

def safe_import(module_name, class_name):
    """Safely import modules with detailed error handling"""
    try:
        module = __import__(module_name, fromlist=[class_name])
        class_obj = getattr(module, class_name)
        return class_obj, True
    except ImportError as e:
        st.warning(f"âŒ Could not import {module_name}: {e}")
        return None, False
    except AttributeError as e:
        st.warning(f"âŒ Could not find {class_name} in {module_name}: {e}")
        return None, False
    except Exception as e:
        st.warning(f"âŒ Unexpected error importing {module_name}.{class_name}: {e}")
        return None, False

# Import available components
FacialExpressionAnalyzer, FACE_ANALYSIS_AVAILABLE = safe_import('facial_analyzer', 'FacialExpressionAnalyzer')
DepressionSeverityPredictor, TEXT_ANALYSIS_AVAILABLE = safe_import('depression_text_predictor', 'DepressionSeverityPredictor')

# Try to import our simplified audio detector
try:
    from audio_depression_detector import AudioDepressionDetector
    AUDIO_ANALYSIS_AVAILABLE = True
except ImportError as e:
    st.warning(f"âŒ Could not import audio_depression_detector: {e}")
    AUDIO_ANALYSIS_AVAILABLE = False

# Initialize predictors with caching
@st.cache_resource
def load_predictor():
    if not TEXT_ANALYSIS_AVAILABLE:
        return None
    try:
        predictor = DepressionSeverityPredictor()
        return predictor
    except Exception as e:
        st.error(f"âŒ Error loading text analysis model: {e}")
        return None

@st.cache_resource
def load_facial_analyzer():
    if not FACE_ANALYSIS_AVAILABLE:
        return None
    try:
        analyzer = FacialExpressionAnalyzer()
        return analyzer
    except Exception as e:
        st.error(f"âŒ Error loading facial analyzer: {e}")
        return None

@st.cache_resource
def load_audio_detector():
    if not AUDIO_ANALYSIS_AVAILABLE:
        return None
    try:
        detector = AudioDepressionDetector()
        return detector
    except Exception as e:
        st.error(f"âŒ Error loading audio detector: {e}")
        return None

def initialize_session_state():
    """Initialize all session state variables"""
    defaults = {
        # Video analysis states
        'video_recording': False,
        'video_results': None,
        'video_analyzing': False,
        'video_recording_started': False,
        'video_stop_clicked': False,
        'recording_start_time': None,
        
        # Video upload states
        'video_upload_analyzing': False,
        'video_upload_processed': False,
        'uploaded_video_file': None,
        
        # Audio analysis states
        'audio_recording': False,
        'audio_results': None,
        'audio_analyzing': False,
        'audio_recording_started': False,
        'audio_stop_clicked': False,
        
        # Audio upload states
        'audio_upload_analyzing': False,
        'audio_upload_processed': False,
        'uploaded_audio_file': None,
        
        # Complete session states
        'complete_session_active': False,
        'complete_session_results': None,
        'complete_session_analyzing': False,
        'complete_session_mode': 'record',  # 'record' or 'upload'
        'complete_session_video_file': None,
        'complete_session_audio_file': None,
        'complete_session_upload_processed': False,
        
        # General states
        'last_action': None,
        'action_timestamp': None
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def main():
    st.title("ğŸ§  Therapy Session Helper - Multi-Modal Analysis")
    st.markdown("Comprehensive emotional analysis through video, audio, and text")
    
    # Initialize session state
    initialize_session_state()
    
    # Load models
    text_predictor = load_predictor()
    facial_analyzer = load_facial_analyzer()
    audio_detector = load_audio_detector()
    
    # Show availability status
    col1, col2, col3 = st.columns(3)
    with col1:
        if text_predictor:
            st.success("âœ… Text analysis available")
        else:
            st.error("âŒ Text analysis unavailable")
    
    with col2:
        if facial_analyzer:
            st.success("âœ… Facial analysis available")
        else:
            st.error("âŒ Facial analysis unavailable")
    
    with col3:
        if audio_detector:
            st.success("âœ… Audio analysis available")
        else:
            st.error("âŒ Audio analysis unavailable")
    
    # Sidebar
    st.sidebar.header("Analysis Options")
    
    # Determine available analysis modes
    available_modes = ["Text Analysis"]
    
    if facial_analyzer:
        available_modes.append("Video Analysis")
    if audio_detector:
        available_modes.append("Audio Analysis")
    if facial_analyzer and audio_detector:
        available_modes.append("Complete Session")
    
    analysis_mode = st.sidebar.radio(
        "Select Analysis Type:",
        available_modes
    )
    
    if analysis_mode == "Text Analysis":
        text_analysis(text_predictor)
    elif analysis_mode == "Audio Analysis":
        audio_analysis(audio_detector)
    elif analysis_mode == "Video Analysis":
        video_analysis(facial_analyzer)
    else:
        complete_session(facial_analyzer, audio_detector)

def video_analysis(facial_analyzer):
    """Video-based emotion analysis with manual recording"""
    st.header("ğŸ¥ Video Emotion Analysis")
    
    if facial_analyzer is None:
        st.error("âŒ Video analysis not available")
        return
    
    # Display usage instructions
    st.subheader("ğŸ“‹ How to Use Video Analysis")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        **ğŸ¯ Step-by-Step Guide:**
        
        1. **ğŸ¬ Start Recording** - Click below to begin video capture
        2. **ğŸ—£ï¸ Speak Naturally** - Express your feelings to the camera
        3. **â¹ï¸ Stop Recording** - Click to end and analyze
        4. **ğŸ“Š View Results** - See facial expression analysis
        
        **ğŸ’¡ Tips for Best Results:**
        - Ensure good lighting on your face
        - Position yourself clearly in frame
        - Speak naturally about your feelings
        - Allow natural facial expressions
        - Record for at least 30 seconds for best analysis
        """)
        
        st.subheader("Recording Controls")
        
        # Handle recording state transitions
        handle_video_recording_states(facial_analyzer)
        
        # Display current status
        display_video_status()
        
    with col2:
        st.subheader("Status")
        display_video_current_status()
        
        st.subheader("Analysis Output")
        st.markdown("""
        - Facial expression percentages
        - Dominant emotions detected
        - Emotional insights
        - Expression timeline
        """)
        
        # Video upload option
        st.subheader("ğŸ“ Upload Video")
        handle_video_upload(facial_analyzer)
    
    # Display recording status and instructions
    if st.session_state.video_recording:
        display_recording_interface()
    
    # Display results if available
    if st.session_state.video_results and not st.session_state.video_analyzing:
        display_video_final_results()

def handle_video_recording_states(facial_analyzer):
    """Handle video recording state transitions"""
    # Start Recording
    if not st.session_state.video_recording and not st.session_state.video_analyzing:
        if st.button("ğŸ¬ Start Video Recording", type="primary", use_container_width=True, key="start_video_recording"):
            try:
                result = facial_analyzer.start_video_recording()
                if isinstance(result, dict) and result.get('status') == 'recording_started':
                    st.session_state.video_recording = True
                    st.session_state.recording_start_time = time.time()
                    st.session_state.video_recording_started = True
                    st.session_state.last_action = "start_recording"
                    st.session_state.action_timestamp = time.time()
                    st.rerun()
                else:
                    st.error("âŒ Could not start video recording")
            except Exception as e:
                st.error(f"âŒ Error starting recording: {e}")
    
    # Stop Recording
    elif st.session_state.video_recording and not st.session_state.video_analyzing:
        if st.button("â¹ï¸ Stop Video Recording", type="secondary", use_container_width=True, key="stop_video_recording"):
            st.session_state.video_recording = False
            st.session_state.video_analyzing = True
            st.session_state.video_stop_clicked = True
            st.session_state.last_action = "stop_recording"
            st.session_state.action_timestamp = time.time()
            st.rerun()
    
    # Handle analysis after stop
    if st.session_state.video_stop_clicked and st.session_state.video_analyzing:
        st.session_state.video_stop_clicked = False
        
        with st.spinner("ğŸ”„ Processing video frames and detecting emotions..."):
            try:
                emotion_results, video_path = facial_analyzer.stop_video_recording()
                if emotion_results is not None:
                    st.session_state.video_results = {
                        'emotions': emotion_results,
                        'video_path': video_path
                    }
                else:
                    st.error("âŒ No emotion data collected from video")
            except Exception as e:
                st.error(f"âŒ Error during video analysis: {e}")
            finally:
                st.session_state.video_analyzing = False
                st.rerun()

def display_video_status():
    """Display current video recording status"""
    if st.session_state.video_recording:
        st.info("ğŸ”´ **Recording in progress** - Speak naturally about your feelings")
    elif st.session_state.video_analyzing:
        st.warning("ğŸ”„ **Analyzing video** - Please wait...")
    else:
        st.success("âœ… **Ready to record**")

def display_video_current_status():
    """Display current status in the right column"""
    if st.session_state.video_recording:
        st.error("ğŸ”´ **Recording Active**")
        st.write("A video window is open. Speak naturally about your feelings.")
    elif st.session_state.video_analyzing:
        st.warning("ğŸ”„ **Analyzing Video**")
        st.write("Processing recorded video...")
    else:
        st.success("âœ… **Ready to Record**")

def handle_video_upload(facial_analyzer):
    """Handle video file upload and analysis - FIXED VERSION"""
    uploaded_video = st.file_uploader("Or upload existing video", 
                                    type=['mp4', 'avi', 'mov'],
                                    help="Upload MP4, AVI, or MOV files for analysis",
                                    key="video_uploader")
    
    # Store uploaded file in session state
    if uploaded_video and uploaded_video != st.session_state.get('uploaded_video_file'):
        st.session_state.uploaded_video_file = uploaded_video
        st.session_state.video_upload_processed = False
    
    # Only show analyze button if we have a file that hasn't been processed
    if (st.session_state.uploaded_video_file and 
        not st.session_state.video_analyzing and 
        not st.session_state.video_upload_analyzing and
        not st.session_state.video_upload_processed):
        
        if st.button("ğŸ­ Analyze Uploaded Video", use_container_width=True, key="analyze_uploaded_video"):
            st.session_state.video_upload_analyzing = True
            st.session_state.video_upload_processed = False
            st.rerun()
    
    # Handle uploaded video analysis - ONLY when explicitly triggered and not already processed
    if (st.session_state.video_upload_analyzing and 
        st.session_state.uploaded_video_file and
        not st.session_state.video_upload_processed):
        
        # Mark as processing immediately to prevent re-triggering
        st.session_state.video_upload_processed = True
        
        with st.spinner(f"ğŸ”„ Analyzing {st.session_state.uploaded_video_file.name}..."):
            try:
                emotion_results = facial_analyzer.analyze_uploaded_video(st.session_state.uploaded_video_file)
                st.session_state.video_results = {
                    'emotions': emotion_results,
                    'video_path': "uploaded_file"
                }
            except Exception as e:
                st.error(f"âŒ Error analyzing video: {e}")
            finally:
                st.session_state.video_upload_analyzing = False
                st.rerun()

def display_recording_interface():
    """Display recording interface when recording is active"""
    st.markdown("---")
    st.subheader("ğŸ¥ Live Recording Active")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        **In the video window:**
        - Look directly at the camera
        - Speak naturally about your feelings
        - Express emotions genuinely
        - Continue for 30+ seconds
        """)
    with col2:
        st.markdown("""
        **Current Analysis:**
        - Real-time face detection
        - Live emotion tracking
        - Expression monitoring
        - Voice recording (if available)
        """)
    
    # Recording timer
    if st.session_state.recording_start_time:
        recording_time = time.time() - st.session_state.recording_start_time
        mins, secs = divmod(int(recording_time), 60)
        st.metric("Recording Duration", f"{mins:02d}:{secs:02d}")

def display_video_final_results():
    """Display final video analysis results"""
    st.markdown("---")
    display_video_results(
        st.session_state.video_results['emotions'],
        "Facial Expression Analysis"
    )
    
    # Additional insights based on results
    emotions = st.session_state.video_results['emotions']
    st.subheader("ğŸ§  Emotional Insights")
    
    if emotions.get('sad', 0) > 25:
        st.warning("**Elevated Sadness**: Higher than typical sadness levels detected in facial expressions")
    elif emotions.get('sad', 0) > 15:
        st.info("**Moderate Sadness**: Some sadness indicators present")
    
    if emotions.get('happy', 0) > 40:
        st.success("**Positive Expression**: Strong positive emotional expressions detected")
    elif emotions.get('happy', 0) > 25:
        st.info("**Balanced Expression**: Good mix of positive expressions")
    
    if emotions.get('neutral', 0) > 70:
        st.info("**Neutral Dominance**: Predominantly neutral emotional expressions")
    
    if emotions.get('fear', 0) > 10 or emotions.get('angry', 0) > 10:
        st.warning("**Elevated Negative Emotions**: Increased fear or anger expressions detected")
    
    # Add a button to clear results and start over
    if st.button("ğŸ”„ New Video Analysis", type="secondary", key="new_video_analysis"):
        reset_video_state()
        st.rerun()

def reset_video_state():
    """Reset all video-related session states"""
    st.session_state.video_results = None
    st.session_state.video_recording = False
    st.session_state.video_analyzing = False
    st.session_state.video_upload_analyzing = False
    st.session_state.video_recording_started = False
    st.session_state.video_stop_clicked = False
    st.session_state.video_upload_processed = False
    st.session_state.uploaded_video_file = None

def audio_analysis(audio_detector):
    """Audio-based depression analysis with manual recording and file upload"""
    st.header("ğŸµ Audio Analysis")
    
    if audio_detector is None:
        st.error("âŒ Audio analysis not available")
        return
    
    # Language selection
    st.subheader("ğŸŒ Language Selection")
    language_choice = st.radio(
        "Select speech language:",
        ["English", "Indonesian"],
        horizontal=True,
        key="audio_language"
    )
    
    # Set the selected language in the detector
    audio_detector.set_language(language_choice.lower())
    
    # Display analysis options
    st.subheader("ğŸ“‹ Analysis Options")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ğŸ™ï¸ Live Recording:**
        - Record your voice in real-time
        - Speak naturally about your feelings
        - Get instant analysis
        """)
        
        st.subheader("Recording Controls")
        handle_audio_recording_states(audio_detector)
    
    with col2:
        st.markdown("""
        **ğŸ“ Upload Audio File:**
        - Upload existing MP3 recordings
        - Analyze pre-recorded sessions
        - Same comprehensive analysis
        """)
        
        st.subheader("File Upload")
        handle_audio_upload(audio_detector)
    
    # Display current status
    display_audio_current_status()
    
    # Display recording timer
    if st.session_state.audio_recording and st.session_state.recording_start_time:
        display_audio_recording_timer()
    
    # Display results if available
    if st.session_state.audio_results and not st.session_state.audio_analyzing:
        display_audio_final_results()
    
    # Display usage tips when idle
    if (not st.session_state.audio_recording and 
        not st.session_state.audio_analyzing and 
        not st.session_state.audio_results):
        display_audio_tips()

def handle_audio_recording_states(audio_detector):
    """Handle audio recording state transitions"""
    # Start Recording
    if not st.session_state.audio_recording and not st.session_state.audio_analyzing:
        if st.button("ğŸ™ï¸ Start Recording", type="primary", use_container_width=True, key="start_audio_recording"):
            result = audio_detector.start_recording()
            if "started" in result:
                st.session_state.audio_recording = True
                st.session_state.recording_start_time = time.time()
                st.session_state.audio_recording_started = True
                st.session_state.last_action = "start_audio_recording"
                st.rerun()
            else:
                st.error("âŒ Could not start recording")
    
    # Stop Recording
    elif st.session_state.audio_recording and not st.session_state.audio_analyzing:
        if st.button("â¹ï¸ Stop Recording", type="secondary", use_container_width=True, key="stop_audio_recording"):
            st.session_state.audio_recording = False
            st.session_state.audio_analyzing = True
            st.session_state.audio_stop_clicked = True
            st.session_state.last_action = "stop_audio_recording"
            st.rerun()
    
    # Handle analysis after stop
    if st.session_state.audio_stop_clicked and st.session_state.audio_analyzing:
        st.session_state.audio_stop_clicked = False
        
        with st.spinner("ğŸ”„ Analyzing audio... This may take a moment."):
            try:
                st.session_state.audio_results = audio_detector.stop_recording()
            except Exception as e:
                st.error(f"âŒ Error during audio analysis: {e}")
            finally:
                st.session_state.audio_analyzing = False
                st.rerun()

def handle_audio_upload(audio_detector):
    """Handle audio file upload and analysis - FIXED VERSION"""
    uploaded_audio = st.file_uploader(
        "Choose audio file", 
        type=['mp3', 'wav', 'm4a'],
        help="Upload MP3, WAV, or M4A files for analysis",
        key="audio_uploader"
    )
    
    # Store uploaded file in session state
    if uploaded_audio and uploaded_audio != st.session_state.get('uploaded_audio_file'):
        st.session_state.uploaded_audio_file = uploaded_audio
        st.session_state.audio_upload_processed = False
    
    # Only show analyze button if we have a file that hasn't been processed
    if (st.session_state.uploaded_audio_file and 
        not st.session_state.audio_analyzing and 
        not st.session_state.audio_upload_analyzing and
        not st.session_state.audio_upload_processed):
        
        if st.button("ğŸ” Analyze Uploaded Audio", type="primary", use_container_width=True, key="analyze_uploaded_audio"):
            st.session_state.audio_upload_analyzing = True
            st.session_state.audio_upload_processed = False
            st.rerun()
    
    # Handle uploaded audio analysis - ONLY when explicitly triggered and not already processed
    if (st.session_state.audio_upload_analyzing and 
        st.session_state.uploaded_audio_file and
        not st.session_state.audio_upload_processed):
        
        # Mark as processing immediately to prevent re-triggering
        st.session_state.audio_upload_processed = True
        
        with st.spinner(f"ğŸ”„ Analyzing {st.session_state.uploaded_audio_file.name}..."):
            try:
                st.session_state.audio_results = audio_detector.analyze_uploaded_audio(st.session_state.uploaded_audio_file)
            except Exception as e:
                st.error(f"âŒ Error analyzing audio: {e}")
            finally:
                st.session_state.audio_upload_analyzing = False
                st.rerun()

def display_audio_current_status():
    """Display current audio analysis status"""
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.session_state.audio_recording:
            st.warning("ğŸ”´ **Currently Recording**")
            st.info("Speak naturally about your feelings. Click 'Stop Recording' when finished.")
        elif st.session_state.audio_analyzing or st.session_state.audio_upload_analyzing:
            st.warning("ğŸ”„ **Analyzing Audio**")
            st.info("Please wait while we analyze your audio...")
        else:
            st.success("âœ… **Ready**")
    
    with col2:
        if st.session_state.uploaded_audio_file:
            st.info(f"ğŸ“ **File Ready**: {st.session_state.uploaded_audio_file.name}")

def display_audio_recording_timer():
    """Display audio recording timer"""
    st.markdown("---")
    st.subheader("ğŸ™ï¸ Recording in Progress...")
    
    recording_time = time.time() - st.session_state.recording_start_time
    mins, secs = divmod(int(recording_time), 60)
    st.metric("Recording Time", f"{mins:02d}:{secs:02d}")
    
    # Show recording tips
    st.info("ğŸ’¡ **Tip**: Speak for at least 30 seconds for best results. Describe your feelings naturally.")

def display_audio_final_results():
    """Display final audio analysis results"""
    st.markdown("---")
    display_audio_results(st.session_state.audio_results)
    
    # Add a button to clear results and start over
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ New Analysis", type="secondary", use_container_width=True, key="new_audio_analysis"):
            reset_audio_state()
            st.rerun()
    with col2:
        if st.button("ğŸ“Š Show Detailed Analysis", use_container_width=True, key="detailed_audio_analysis"):
            st.info("Detailed analysis features coming soon!")

def reset_audio_state():
    """Reset all audio-related session states"""
    st.session_state.audio_results = None
    st.session_state.audio_recording = False
    st.session_state.audio_analyzing = False
    st.session_state.audio_upload_analyzing = False
    st.session_state.audio_recording_started = False
    st.session_state.audio_stop_clicked = False
    st.session_state.audio_upload_processed = False
    st.session_state.uploaded_audio_file = None

def display_audio_tips():
    """Display audio analysis tips"""
    st.markdown("---")
    st.subheader("ğŸ’¡ Tips for Best Results")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        **For Live Recording:**
        - Speak clearly and naturally
        - Record in a quiet environment
        - Speak for 1-3 minutes
        - Express genuine emotions
        - Use normal speaking volume
        """)
    
    with col2:
        st.markdown("""
        **For File Upload:**
        - MP3 format recommended
        - Clear audio quality
        - Minimum 30 seconds duration
        - Single speaker preferred
        - Minimal background noise
        """)

def text_analysis(predictor):
    """Text-based depression analysis"""
    st.header("ğŸ“ Text Analysis")
    
    if predictor is None:
        st.error("âŒ Text analysis model not available")
        return
    
    text_input = st.text_area(
        "Enter text to analyze:",
        height=150,
        placeholder="Describe how you're feeling or paste text to analyze...",
        key="text_input"
    )
    
    if st.button("Analyze Text", type="primary", key="analyze_text") and text_input:
        with st.spinner("Analyzing text content..."):
            try:
                # Use the available method
                if hasattr(predictor, 'predict_severity'):
                    result = predictor.predict_severity(text_input)
                elif hasattr(predictor, 'predict'):
                    result = predictor.predict([text_input])[0]
                else:
                    st.error("No prediction method available")
                    return
                
                display_text_results(result)
            except Exception as e:
                st.error(f"âŒ Analysis error: {e}")

def complete_session(facial_analyzer, audio_detector):
    """Complete multi-modal analysis session - Both recording and video upload options"""
    st.header("ğŸ’¬ Complete Therapy Session")
    
    if not facial_analyzer or not audio_detector:
        st.error("âŒ Both video and audio analysis required for complete session")
        return
    
    st.info("""
    **Complete Multi-Modal Analysis:**
    - **ğŸ¥ Video Analysis**: Facial expression analysis
    - **ğŸµ Audio Analysis**: Voice and speech content analysis  
    - **ğŸ§  Combined**: Comprehensive emotional assessment
    
    **Choose your method:**
    - **ğŸ¥ Record Live**: Record video with audio simultaneously
    - **ğŸ“ Upload Video**: Upload existing video file for analysis
    """)
    
    # Language selection for complete session
    st.subheader("ğŸŒ Language Selection")
    language_choice = st.radio(
        "Select speech language:",
        ["English", "Indonesian"],
        horizontal=True,
        key="complete_session_language"
    )
    
    # Set the selected language in the audio detector
    audio_detector.set_language(language_choice.lower())
    
    # Session mode selection
    st.subheader("ğŸ¯ Session Mode")
    session_mode = st.radio(
        "Select session mode:",
        ["Record Live Session", "Upload Video File"],
        horizontal=True,
        key="session_mode"
    )
    
    # Store session mode in state
    st.session_state.complete_session_mode = 'record' if session_mode == "Record Live Session" else 'upload'
    
    if st.session_state.complete_session_mode == 'record':
        handle_live_complete_session(facial_analyzer, audio_detector, language_choice)
    else:
        handle_upload_complete_session(facial_analyzer, audio_detector, language_choice)
    
    # Display results if available
    if st.session_state.complete_session_results and not st.session_state.complete_session_analyzing:
        display_complete_session_results()

def handle_live_complete_session(facial_analyzer, audio_detector, language_choice):
    """Handle live recording for complete session"""
    st.subheader("ğŸ¥ğŸ™ï¸ Live Recording Session")
    
    st.markdown("""
    **Live Recording Instructions:**
    1. Click **Start Complete Session** to begin recording
    2. A video window will open - speak naturally about your feelings
    3. Both video and audio will be recorded simultaneously
    4. Click **Stop Complete Session** when finished
    5. Get comprehensive analysis from both modalities
    """)
    
    # Complete session controls
    st.subheader("ğŸ¯ Session Controls")
    
    if not st.session_state.complete_session_active and not st.session_state.complete_session_analyzing:
        if st.button("ğŸš€ Start Complete Session", type="primary", use_container_width=True):
            # Start both video and audio recording
            try:
                # Start video recording
                video_result = facial_analyzer.start_video_recording()
                # Start audio recording
                audio_result = audio_detector.start_recording()
                
                if (isinstance(video_result, dict) and video_result.get('status') == 'recording_started' and
                    "started" in audio_result):
                    st.session_state.complete_session_active = True
                    st.session_state.recording_start_time = time.time()
                    st.session_state.last_action = "start_complete_session"
                    st.rerun()
                else:
                    st.error("âŒ Could not start complete session")
            except Exception as e:
                st.error(f"âŒ Error starting complete session: {e}")
    
    elif st.session_state.complete_session_active and not st.session_state.complete_session_analyzing:
        if st.button("â¹ï¸ Stop Complete Session", type="secondary", use_container_width=True):
            st.session_state.complete_session_active = False
            st.session_state.complete_session_analyzing = True
            st.session_state.last_action = "stop_complete_session"
            st.rerun()
    
    # Handle analysis after stopping complete session
    if st.session_state.complete_session_analyzing and st.session_state.complete_session_mode == 'record':
        with st.spinner("ğŸ”„ Analyzing complete session data... This may take a moment."):
            try:
                # Stop both recordings and get results
                video_emotions, video_path = facial_analyzer.stop_video_recording()
                audio_results = audio_detector.stop_recording()
                
                # Combine results
                combined_results = {
                    'video_emotions': video_emotions,
                    'audio_results': audio_results,
                    'combined_score': calculate_combined_score(video_emotions, audio_results),
                    'timestamp': datetime.now().isoformat(),
                    'language': language_choice.lower(),
                    'mode': 'live'
                }
                
                st.session_state.complete_session_results = combined_results
                st.session_state.complete_session_analyzing = False
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ Error during complete session analysis: {e}")
                st.session_state.complete_session_analyzing = False
                st.rerun()
    
    # Display current status
    display_complete_session_status()
    
    # Display recording timer if active
    if st.session_state.complete_session_active and st.session_state.recording_start_time:
        display_complete_session_timer()

def handle_upload_complete_session(facial_analyzer, audio_detector, language_choice):
    """Handle video upload for complete session - extracts both video and audio from single file"""
    st.subheader("ğŸ“ Upload Video for Complete Analysis")
    
    st.markdown("""
    **Video Upload Instructions:**
    - Upload a video file containing both visual and audio content
    - We'll extract facial expressions from video frames
    - We'll extract and analyze the audio track from the video
    - Get comprehensive analysis from both modalities
    """)
    
    uploaded_video = st.file_uploader(
        "Upload video file for complete analysis",
        type=['mp4', 'avi', 'mov', 'mkv'],
        help="Upload a video file with both visual and audio content",
        key="complete_session_video"
    )
    
    # Store uploaded file in session state
    if uploaded_video and uploaded_video != st.session_state.get('complete_session_video_file'):
        st.session_state.complete_session_video_file = uploaded_video
        st.session_state.complete_session_upload_processed = False
    
    # Show analyze button when video is uploaded
    if (st.session_state.complete_session_video_file and 
        not st.session_state.complete_session_analyzing and
        not st.session_state.complete_session_upload_processed):
        
        if st.button("ğŸ” Run Complete Analysis", type="primary", use_container_width=True):
            st.session_state.complete_session_analyzing = True
            st.session_state.complete_session_upload_processed = False
            st.rerun()
    
    # Handle complete session analysis for upload
    if (st.session_state.complete_session_analyzing and 
        st.session_state.complete_session_mode == 'upload' and
        st.session_state.complete_session_video_file and
        not st.session_state.complete_session_upload_processed):
        
        # Mark as processing immediately to prevent re-triggering
        st.session_state.complete_session_upload_processed = True
        
        with st.spinner("ğŸ”„ Running complete multi-modal analysis..."):
            try:
                # Save the uploaded video to a temporary file
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_video:
                    temp_video.write(st.session_state.complete_session_video_file.read())
                    video_path = temp_video.name
                
                # Step 1: Analyze video for facial expressions
                st.info("ğŸ¥ Analyzing facial expressions...")
                video_emotions = facial_analyzer.process_video_file(video_path)
                
                # Step 2: Extract and analyze audio from the video
                st.info("ğŸµ Extracting and analyzing audio...")
                audio_results = extract_and_analyze_audio_from_video(audio_detector, video_path, language_choice.lower())
                
                # Step 3: Combine results
                combined_results = {
                    'video_emotions': video_emotions,
                    'audio_results': audio_results,
                    'combined_score': calculate_combined_score(video_emotions, audio_results),
                    'timestamp': datetime.now().isoformat(),
                    'language': language_choice.lower(),
                    'mode': 'video_upload',
                    'filename': st.session_state.complete_session_video_file.name
                }
                
                st.session_state.complete_session_results = combined_results
                
                # Clean up temporary file
                try:
                    os.unlink(video_path)
                except:
                    pass
                
            except Exception as e:
                st.error(f"âŒ Error during complete session analysis: {e}")
            finally:
                st.session_state.complete_session_analyzing = False
                st.rerun()
    
    # Display current status for upload mode
    display_complete_session_status()

def extract_and_analyze_audio_from_video(audio_detector, video_path, language):
    """Extract audio from video and analyze it"""
    try:
        # Create temporary audio file
        temp_audio_path = tempfile.NamedTemporaryFile(delete=False, suffix='.wav').name
        
        # Extract audio from video using pydub
        from pydub import AudioSegment
        
        # Load video and extract audio
        video = AudioSegment.from_file(video_path, format="mp4")
        
        # Export as WAV for analysis
        video.export(temp_audio_path, format="wav")
        
        # Analyze the extracted audio
        audio_results = audio_detector.analyze_audio(temp_audio_path)
        
        # Clean up temporary audio file
        try:
            os.unlink(temp_audio_path)
        except:
            pass
        
        return audio_results
        
    except Exception as e:
        st.error(f"âŒ Error extracting audio from video: {e}")
        # Return fallback audio results
        return {
            'phq_score': 10,
            'acoustic_score': 10,
            'semantic_score': 10,
            'depression_percentage': 37.0,
            'transcript': 'Audio extraction failed',
            'acoustic_insights': ['Could not analyze audio features'],
            'semantic_insights': ['Could not analyze speech content']
        }

def calculate_combined_score(video_emotions, audio_results):
    """Calculate combined emotional score from video and audio analysis"""
    try:
        # Video component: focus on sadness and negative emotions
        video_score = 0
        if video_emotions:
            sadness = video_emotions.get('sad', 0)
            fear = video_emotions.get('fear', 0)
            angry = video_emotions.get('angry', 0)
            
            # Convert emotion percentages to score (0-27 scale)
            video_score = (sadness * 0.15 + fear * 0.1 + angry * 0.05) * 0.27
        
        # Audio component: use PHQ-like score
        audio_score = audio_results.get('phq_score', 0) if audio_results else 0
        
        # Combine with weights (adjust as needed)
        combined = (video_score * 0.4) + (audio_score * 0.6)
        return min(27, combined)
        
    except Exception as e:
        st.error(f"Error calculating combined score: {e}")
        return 0

def display_complete_session_status():
    """Display current complete session status"""
    st.markdown("---")
    
    if st.session_state.complete_session_analyzing:
        st.warning("ğŸ”„ **Analyzing Complete Session**")
        if st.session_state.complete_session_mode == 'record':
            st.info("Processing live recording data...")
        else:
            st.info("Processing uploaded video file...")
    elif st.session_state.complete_session_active:
        st.error("ğŸ”´ **Complete Session Active**")
        st.info("Both video and audio recording are in progress. Speak naturally about your feelings.")
    elif st.session_state.complete_session_video_file and not st.session_state.complete_session_results:
        st.success(f"âœ… **Ready to Analyze**: {st.session_state.complete_session_video_file.name}")
    elif st.session_state.complete_session_results:
        st.success("âœ… **Analysis Complete**")
    else:
        mode = "Live Recording" if st.session_state.complete_session_mode == 'record' else "Video Upload"
        st.info(f"ğŸ¯ **Ready for {mode}**")

def display_complete_session_timer():
    """Display complete session recording timer"""
    st.markdown("---")
    st.subheader("ğŸ¥ğŸ™ï¸ Complete Session Recording...")
    
    recording_time = time.time() - st.session_state.recording_start_time
    mins, secs = divmod(int(recording_time), 60)
    st.metric("Session Duration", f"{mins:02d}:{secs:02d}")
    
    # Show recording tips
    st.info("""
    ğŸ’¡ **Complete Session Tips:**
    - Speak naturally about your feelings
    - Maintain eye contact with the camera
    - Express emotions genuinely
    - Continue for 1-3 minutes for best results
    - Describe both thoughts and feelings
    """)

def display_complete_session_results():
    """Display complete session analysis results"""
    st.markdown("---")
    st.header("ğŸ“Š Complete Session Analysis")
    
    results = st.session_state.complete_session_results
    
    # Show mode information
    if results.get('mode') == 'upload':
        st.info(f"ğŸ“ **Analyzed File:** {results.get('filename', 'Unknown')}")
    else:
        st.info("ğŸ¥ **Live Recording Analysis**")
    
    # Overall score
    combined_score = results.get('combined_score', 0)
    depression_percentage = (combined_score / 27) * 100
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Overall Depression Score", f"{combined_score:.1f}/27")
    
    with col2:
        st.metric("Depression Percentage", f"{depression_percentage:.1f}%")
    
    with col3:
        severity = "Unknown"
        if combined_score < 5:
            severity = "Minimal"
        elif combined_score < 10:
            severity = "Mild"
        elif combined_score < 15:
            severity = "Moderate"
        elif combined_score < 20:
            severity = "Moderately Severe"
        else:
            severity = "Severe"
        st.metric("Overall Severity", severity)
    
    # Component analysis
    st.subheader("ğŸ” Component Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("### ğŸ¥ Video Analysis")
        if results.get('video_emotions'):
            display_video_results(results['video_emotions'], "Facial Expressions")
        else:
            st.warning("No video analysis data available")
    
    with col2:
        st.write("### ğŸµ Audio Analysis")
        if results.get('audio_results'):
            audio_results = results['audio_results']
            st.write(f"**PHQ-like Score:** {audio_results.get('phq_score', 0):.1f}/27")
            st.write(f"**Acoustic Score:** {audio_results.get('acoustic_score', 0):.1f}/27")
            st.write(f"**Semantic Score:** {audio_results.get('semantic_score', 0):.1f}/27")
            
            # Show key insights
            st.write("**Voice Insights:**")
            for insight in audio_results.get('acoustic_insights', [])[:3]:
                st.write(f"â€¢ {insight}")
            
            st.write("**Content Insights:**")
            for insight in audio_results.get('semantic_insights', [])[:3]:
                st.write(f"â€¢ {insight}")
                
            # Show transcript if available
            if audio_results.get('transcript') and audio_results.get('transcript') not in ['Speech recognition unavailable', 'Analysis failed']:
                with st.expander("View Speech Transcript"):
                    st.write(audio_results['transcript'])
        else:
            st.warning("No audio analysis data available")

    # Add this function after display_complete_session_results function

def generate_combined_insights(results):
    """Generate insights from combined video and audio analysis"""
    insights = []
    
    video_emotions = results.get('video_emotions', {})
    audio_results = results.get('audio_results', {})
    combined_score = results.get('combined_score', 0)
    
    # Video-based insights
    sadness_video = video_emotions.get('sad', 0)
    if sadness_video > 25:
        insights.append("âš ï¸ **High facial sadness**: Elevated sadness detected in facial expressions")
    elif sadness_video > 15:
        insights.append("â„¹ï¸ **Moderate facial sadness**: Some sadness indicators in expressions")
    
    neutral_video = video_emotions.get('neutral', 0)
    if neutral_video > 70:
        insights.append("ğŸ˜ **Flat affect**: Predominantly neutral facial expressions detected")
    
    # Audio-based insights
    audio_score = audio_results.get('phq_score', 0)
    if audio_score > 20:
        insights.append("âš ï¸ **Severe vocal indicators**: Strong depression indicators in speech patterns")
    elif audio_score > 15:
        insights.append("â„¹ï¸ **Moderate vocal indicators**: Some depression indicators in speech")
    
    # Combined insights
    if combined_score > 20:
        insights.append("ğŸš¨ **High overall concern**: Multiple indicators suggest significant emotional distress")
    elif combined_score > 15:
        insights.append("âš ï¸ **Moderate overall concern**: Several indicators of emotional distress present")
    elif combined_score < 5:
        insights.append("âœ… **Positive indicators**: Minimal distress signals detected across modalities")
    
    # Consistency check
    if (sadness_video > 20 and audio_score > 15):
        insights.append("ğŸ” **Consistent indicators**: Both facial and vocal analysis show consistent emotional patterns")
    
    return insights if insights else ["ğŸ“Š **Baseline analysis**: Standard emotional patterns detected"]

def display_recommendations(combined_score, results):
    """Display recommendations based on complete session analysis"""
    if combined_score >= 20:
        st.error("""
        **ğŸš¨ Immediate Recommendation:**
        - Consider consulting with a mental health professional
        - Reach out to support networks
        - Practice self-care and stress management
        - Consider crisis resources if needed
        """)
    elif combined_score >= 15:
        st.warning("""
        **âš ï¸ Moderate Concern Recommendation:**
        - Monitor emotional patterns regularly
        - Consider speaking with a counselor
        - Practice mindfulness and relaxation techniques
        - Maintain social connections
        """)
    elif combined_score >= 10:
        st.info("""
        **â„¹ï¸ Mild Concern Recommendation:**
        - Continue self-monitoring
        - Practice stress management
        - Maintain healthy routines
        - Consider preventive mental health practices
        """)
    else:
        st.success("""
        **âœ… Maintenance Recommendation:**
        - Continue current emotional wellness practices
        - Regular self-check-ins
        - Maintain healthy lifestyle habits
        - Continue social engagement
        """)
    
    # Combined insights
    st.subheader("ğŸ§  Combined Emotional Insights")
    
    # Generate insights based on combined data
    insights = generate_combined_insights(results)
    for insight in insights:
        if "warning" in insight.lower() or "severe" in insight.lower():
            st.warning(insight)
        elif "positive" in insight.lower() or "good" in insight.lower():
            st.success(insight)
        else:
            st.info(insight)
    
    # Recommendation
    st.subheader("ğŸ’¡ Recommendations")
    display_recommendations(combined_score, results)
    
    # Reset button
    if st.button("ğŸ”„ New Complete Session", type="secondary", use_container_width=True):
        reset_complete_session_state()
        st.rerun()

def reset_complete_session_state():
    """Reset complete session states"""
    st.session_state.complete_session_active = False
    st.session_state.complete_session_results = None
    st.session_state.complete_session_analyzing = False
    st.session_state.complete_session_upload_processed = False
    st.session_state.complete_session_video_file = None

def display_audio_results(results):
    """Display audio analysis results"""
    st.subheader("ğŸµ Audio Analysis Results")
    
    if 'error' in results:
        st.error(f"Analysis error: {results['error']}")
        return
    
    # Main metrics
    col1, col2, col3 = st.columns(3)
    
    with col1:
        depression_pct = results.get('depression_percentage', 0)
        st.metric("Depression Percentage", f"{depression_pct:.1f}%")
    
    with col2:
        phq_score = results.get('phq_score', 0)
        st.metric("PHQ-like Score", f"{phq_score:.1f}/27")
    
    with col3:
        severity = "Unknown"
        phq_score = results.get('phq_score', 0)
        if phq_score < 5:
            severity = "Minimal"
        elif phq_score < 10:
            severity = "Mild"
        elif phq_score < 15:
            severity = "Moderate"
        elif phq_score < 20:
            severity = "Moderately Severe"
        else:
            severity = "Severe"
        st.metric("Severity Level", severity)
    
    # Component scores
    st.subheader("Component Analysis")
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Acoustic Analysis**")
        st.write(f"Score: {results.get('acoustic_score', 0):.1f}/27")
        
        st.write("**Voice Insights:**")
        for insight in results.get('acoustic_insights', []):
            st.write(f"â€¢ {insight}")
    
    with col2:
        st.write("**Semantic Analysis**")
        st.write(f"Score: {results.get('semantic_score', 0):.1f}/27")
        
        st.write("**Content Insights:**")
        for insight in results.get('semantic_insights', []):
            st.write(f"â€¢ {insight}")
    
    # Transcript
    if results.get('transcript'):
        with st.expander("View Speech Transcript"):
            st.write(results['transcript'])
    
    # Language info
    if results.get('selected_language'):
        st.info(f"ğŸŒ Analysis performed in: {results['selected_language'].title()}")
    
    # Visualization
    st.subheader("Score Distribution")
    scores_data = {
        'Component': ['Acoustic', 'Semantic', 'Combined'],
        'Score': [
            results.get('acoustic_score', 0),
            results.get('semantic_score', 0), 
            results.get('phq_score', 0)
        ]
    }
    df_scores = pd.DataFrame(scores_data)
    
    fig = px.bar(df_scores, x='Component', y='Score', 
                 title="Depression Score Components",
                 color='Component',
                 range_y=[0, 27])
    st.plotly_chart(fig, use_container_width=True)
    
    # Additional info
    if results.get('analysis_timestamp'):
        st.caption(f"Analysis performed at: {results['analysis_timestamp']}")

def display_video_results(emotion_results, question):
    """Display video analysis results"""
    st.subheader("ğŸ­ Facial Expression Analysis")
    
    if not emotion_results:
        st.warning("No emotion data collected")
        return
    
    # Convert to DataFrame for plotting
    emotions_data = []
    for emotion, percentage in emotion_results.items():
        emotions_data.append({
            'Emotion': emotion.capitalize(),
            'Percentage': percentage
        })
    
    df = pd.DataFrame(emotions_data)
    
    # Create bar chart
    fig = px.bar(df, x='Emotion', y='Percentage', 
                 title="Facial Expression Distribution",
                 color='Emotion')
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Key metrics
    col1, col2, col3 = st.columns(3)
    
    dominant_emotion = max(emotion_results.items(), key=lambda x: x[1])
    
    with col1:
        st.metric("Dominant Emotion", dominant_emotion[0].capitalize())
    
    with col2:
        st.metric("Dominant Confidence", f"{dominant_emotion[1]:.1f}%")
    
    with col3:
        sadness_pct = emotion_results.get('sad', 0)
        st.metric("Sadness Level", f"{sadness_pct:.1f}%")
        
        # Color code based on sadness level
        if sadness_pct > 25:
            st.error("High sadness level detected")
        elif sadness_pct > 15:
            st.warning("Moderate sadness level detected")
    
    # Emotional insights
    st.subheader("Emotional Insights")
    if emotion_results.get('sad', 0) > 20:
        st.warning("Elevated sadness detected in facial expressions")
    if emotion_results.get('happy', 0) > 30:
        st.success("Positive emotional expressions detected")
    if emotion_results.get('neutral', 0) > 60:
        st.info("Predominantly neutral emotional expressions")

def display_text_results(result):
    """Display text analysis results"""
    st.subheader("ğŸ“Š Text Analysis Results")
    
    if 'error' in result:
        st.error(f"Analysis error: {result['error']}")
        return
    
    # Main result
    severity = result.get('severity', 'unknown').capitalize()
    confidence = result.get('confidence', 0)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Detected Severity", severity)
    
    with col2:
        st.metric("Confidence", f"{confidence:.2f}")
    
    # Probabilities chart
    if 'probabilities' in result:
        probabilities = result['probabilities']
        
        prob_df = pd.DataFrame({
            'Severity': [s.capitalize() for s in probabilities.keys()],
            'Probability': probabilities.values()
        })
        
        fig = px.bar(prob_df, x='Severity', y='Probability',
                     title="Severity Probability Distribution",
                     color='Severity')
        st.plotly_chart(fig, use_container_width=True)
    
    # Additional info
    if result.get('was_translated'):
        st.info("ğŸŒ Text was translated from Indonesian to English for analysis")
    
    if result.get('detected_language'):
        st.write(f"**Detected Language:** {result['detected_language'].upper()}")

if __name__ == "__main__":
    main()